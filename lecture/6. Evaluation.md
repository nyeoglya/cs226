### 6-1. Introduction
- 1000명 당 1개 꼴로 발생하는 질병이 있다고 하자.
- 항상 클래스를 negative로 분류하는 자명한 분류기는 accuracy 99.9%이지만, recall은 0이다.
- 달리 말해, accuracy는 class skew와 cost를 무시한다.
- 올바르지 않은 metric은 잘못된 모델 선택을 이끈다.
- learning curve를 보면 더 많은 데이터가 모델을 개선할지 아닌지도 알 수 있다.
- 우리의 목적은 관측하지 않은 데이터에 대해, 비편향된 추론을 하는 것이다.
### 6-2. train/test
- leakage를 막기 위해, 데이터를 training/test로 쪼갠다.
- 모델은 training에서만 학습시킨다. 평가는 test에서만 한다.
- 최종 모델 생성을 위해 test set을 마지막에 딱 1번만 학습시킨다.
- learning curve: performance vs training size를 그려서 scaling behavior 관찰
	- 가로 축은 train set 크기, 세로 축은 accuracy를 넣는다.
- stratified sampling: train/test 데이터 분할에 클래스 비율을 유지한다. 
- k-fold cross-validation
	- 데이터를 k개로 쪼개고, 정확히 k개 experiment를 수행한다.
	- 각 experiment i마다 i를 제외한 나머지로 학습시키고, i로 테스트한다.
	- cross-validation은 k회 experiment 수행한 평균 metric를 반환한다.
### 6-3. confusion matrix
**(정답인지)(모델의예측)**

|                | **Predicted-Pos** | **Predicted-Neg** |
| :------------: | :---------------: | :---------------: |
| **Actual-Pos** |        TP         |        FN         |
| **Actual-Neg** |        FP         |        TN         |
	- 이를 이용해서 아래와 같은 여러 metric을 정의할 수 있다.
$$
\text{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN},\quad \text{Error}=1-\text{Accuracy}
$$
$$
\text{Recall}(TPR)=\frac{TP}{TP+FN},\quad \text{Precision}(PPV)=\frac{TP}{TP+FP}
$$
$$
FPR=\frac{FP}{FP+TN},\quad \text{Specificity}=1-FPR
$$
- 각 metric에 대한 설명은 아래와 같다.
	- accuracy: 전체 측정 중에서 모델의 예측값과 실제값이 똑같은 비율.
	- error: 모델의 예측값과 실제값이 다른 비율.
	- recall(TPR): 실제로 True인 것 중에서 모델이 True로 예측한 비율.
	- precision(PPV): 모델이 True로 예측한 것 중에서 실제로 True인 것의 비율.
	- FPR: 실제로 False인 것 중에서 모델이 False로 예측한 비율.
	- specificity: 모델이 False로 예측한 것 중에서 실제로 False인 것의 비율.
- ROC: FPR를 x축, TPR를 y축으로 그린 그림. 대각선에 가까울수록 랜덤한 예측이고, top-left에 가까울수록 좋은 예측임.
	- 예측 신뢰도 $c_i$를 가진 데이터 $(x_i, y_i)$가 주어지면, 이를 $c_i$에 대해 내림차순 정렬한다.
	- 데이터를 순차적으로 처리하면서 TPR, FPR를 계산한다.
	- 임계값 $c_i$가 바뀔 때마다 점을 찍으면, ROC를 그릴 수 있다.
- precision-recall (PR) curve: x축이 recall, y축이 precision이 되도록 그리는 곡선.
	- threshold를 변화시키며, 점들을 찍는다.
	- positive 값이 드문 경우, 더 많은 정보를 제공한다. precision이 false positive에 민감하기때문이다.
	- baseline(random ranking): precision이 positive base rate와 같은 경우. 모델의 성능이 이 기준점 위에 있어야 의미가 있는 모델임.
	- PR-AUC: PR 곡선 아래의 넓이.
### 6-4. Choosing Metric
- 균형잡힌 클래스: ROC AUC, Accuracy, F1
- 불균형한 클래스: PR AUC, precision@k(상위 k개 중 TP의 비율), recall at a clinically useful threshold(내가 원하는 recall을 특정 오차 이내로 달성하는 임계값 선택하기)
