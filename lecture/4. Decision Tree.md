### 4-1. Decision Tree
- decision tree: model that recursively partition the feature space with *predicated-based splits*.
- inferenced:
	- internal node: hold test. (evaluation based on the predicates)
	- leaf node: output constant prediction
- At each node, *we want to find a good(=local optimum) split* (type of a greedy) that ==minimize weighted impurity of children (VERY IMPORTANT)==
- 여기서는 binary branching에 대해서만 다룬다.
- KD-tree는 $(X,y)\in\mathbb{R}^{d}\times\mathcal{C}$에서 $X\in\mathbb{R}^d$만 신경쓴다.
- 만약 우리가 classification 작업을 하고 싶다면? → $y \in \mathcal{C}$도 신경써야 한다.
	- classification: 특정 기준에 대해 여러번 branching을 하고, 마지막 leaf 노드에 예측값을 넣어놓는다. 그 branching을 계속해서 leaf 노드를 만나면 그게 예측값이 되는 것이다. → interpretable model임(해석이 되니까)
	- regression하고 싶으면 그냥 가중 평균 취하면 됨.
- 시간복잡도를 줄이기 위해, 데이터를 sort → partition한다. (휴리스틱)
	- 그냥 나누면 $2^n$개 가능성이 있지만, 정렬하고 특정 선을 기준으로 분할하면 $n$가지 가능성 밖에 없다.
- 데이터가 sort가 안되는 경우는 좀 문제가 있다.
### 4-2. Entropy
- depth를 늘리면 아무거나 다 fit할 수 있긴 한데, overfitting 문제가 생긴다. 따라서, 적절한 사이즈로만 나누는게 중요함.
- good decision: 노드가 만약 하나의 classification만 있다면 굉장히 좋음(certain). 만약 여러 개의 classification이 있다면 그리 좋지는 않음(uncertain).
	- 하나의 노드 안에서, 랜덤하게 point를 골랐을 때, 어떤 걸 고를지 확률을 계산할 수 있다. (비율의 형태로) → Entropy. (uncertainty를 계산하는 방법)
	- leaf node에서는 uncertainty가 없어야 된다. root node에서는 아주 uncertain하지만, leaf에서는 아주 certain해야 한다.
- entropy: 확률분포 $\hat{P}$의 uncertainty를 측정하는 척도.
	- (여기서 hat은 training set으로 추정한 확률분포라는 뜻)
	- 때때로 확률분포와 연관된 random variable $Y$를 사용하기도 한다.
$$
H(Y)=\sum_{y \in Y} p(y) \log_2{\frac{1}{p(y)}}=-\sum_{y \in Y} p(y) \log_2{p(y)}
$$
- 여기서 $\log{\frac{1}{p(y)}}$는 뭘 의미할까? uniform distribution을 가정하면 이건 일종의 bit length(information length)이다(k-bit 데이터에 대해 $p(y)=1/2^k$이기 때문).
	- 전체적으로 average information length로 해석할 수 있다.
- uniform distribution에서는 매우 uncertain하다. 아무거나 다 일어날 수 있음. 이 경우를 impure하다고 한다.
- 특정 peak가 있는 distribution은 매우 certain하다. 특정 케이스만 일어남. 이건 매우 pure하다. 이 경우에, 엔트로피가 감소한다. (information의 농도가 높음)
- conditional entropy: 특정 조건 하에서 entropy임.
$$
H(Y|X=x)=-\sum_{y\in Y}p(y|x)\cdot\log p(y|x)
$$
$$
H(Y|X)=E_x[H[Y|X=x]]=\sum_{x\in X}p(x)\cdot H(Y|x)
$$
- 하나의 조건에 대해 정의하고, 평균낸거임. 그냥, 일부 파트를 보고 엔트로피 계산하면 됨. (==검증해보기==)
- Information Gain: $I(Y;X)=H(Y)-H(Y|X)$
	- 전체 엔트로피에서 내 조건을 가졌을 때의 엔트로피 빼기. 값이 클 수록 많은 정보를 얻은 것이라 생각할 수 있다.
- information gain을 최대화하는 것을 목적으로 하자. = $H(Y|X)$를 최소화하는 것을 목적으로 하자. ($H(Y)는$ 고정된 값이기 때문)
- GINI impurity: k-class라고 가정하면 아래과 같이 계산된다. 여기서 $P_A$는 space $S$의 subspace $A$에서의 estimate한 특정 classification의 확률임 (비율의 관점에서)
$$
G(P_S) = 1-\sum_{c=1}^k P_c^2
$$
	- label mismatch probability임
- independent하게 추출한 두 레이블 $Y_1$, $Y_2$에 대해, 다음이 성립함을 명심하자. 이걸로 GINI impurity에 대한 직관적인 이해를 얻을 수 있다. (왜 label mismatch인지)
$$
P(Y_1=Y_2)=\sum_{c=1}^k P(Y_1=c,Y_2=c)=\sum{P_c P_c}=\sum{P_c^2}
$$
- weighted GINI impurity: left, right로 분리한다고 가정할 때, 다음과 같다.
$$
\frac{|S_l|}{|S|}G(P_{S_{l}})+\frac{|S_r|}{|S|}G(P_{S_{r}})
$$
- 위의 weighted GINI impurity를 minimize하는 argument를 찾는 것을 목적으로 한다.
- 이걸 conditional entropy와 연결할 수 있다..? (==확인해보기==)
