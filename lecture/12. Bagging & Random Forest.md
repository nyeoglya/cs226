### 12-1. Ensemble Methods
- $(x_i,y_i)$ 데이터셋을 이용해 이진 분류를 한다고 하자.
- 우리의 목적은, 데이터셋 뿐만 아니라 새로운 데이터에 대해서도 잘 예측을 수행하는 predictor $h$를 만드는 것이다.
- recall: decision tree는 각 단계에서 feature $X_j$를 특정 threshold를 기준으로 분할하고, 예측의 경우 해당 region 내의 target들의 가중 평균으로 prediction을 수행한다.
- overfit(high variance): decision tree는 깊이가 매우 깊어질 수 있다. noise를 많이 capture한다. 이건 overfitting임.
	- 이걸 막기 위해서는 여러 regularization 기술이 들어간다.
	- 깊이를 제한하거나, leaf의 수를 제한하거나, leaf 당 가져야 하는 최소 sample의 수를 만드는 방식 등이 사용된다.
	- 이러한 기법은 variance를 줄이지만, bias는 늘린다.
	- 즉, 과적합은 덜 되지만, 역으로 성능이 떨어진다.
- averaging: variance를 줄이는 기법.
	- $i$번째 모델의 예측 $Z_i$가 있다고 하자. 각 $Z_i$가 i.i.d라고 가정하자.
	- 이렇게 얻은 샘플을 평균내면 variance가 줄어든다.
	- 수학적으로 크기 $n$인 샘플의 variance가 $Var(\bar{Z})=\sigma^2/n$인 것과 같은 이치이다.
	- 만약 i.i.d가 아니라면 covariance term이 추가된다. 이는 $Z_i$가 positive correlated되는 경우에 발생하며, 이러한 경우에는 데이터를 늘렸을 때, variance reduction되는 정도가 줄어든다.
### 12-2. Bagging
- bagging(bootstrap aggregation): decision tree를 random object로 본다.
	- 데이터를 조금만 purterb해도 완전 다른 tree를 얻을 수 있다.
	- 분포 P에서 얻은 독립적인 데이터셋들에 대해 tree를 각각 학습할 수 있으면, 상당히 많은 variance 감소를 얻을 수 있다.
	- bagging은 데이터셋이 1개로 주어졌을 때, 이러한 상황을 모사하는 방법이다.
- bootstrapping: 데이터셋 D에서 중복을 허용해서 n개의 데이터를 샘플링한다.
	- 이러한 기법은 하나의 데이터셋만 가지고 해당 데이터셋과 같은 분포에서 독립적인 데이터셋들을 여러 개 얻는 상황을 모사한다.
	- OOB(out-of-bag) samples: 운이 없어서 아예 안 뽑히는 경우. n번의 시행에서 $(1-1/n)^n \approx 1/e$ 정도의 데이터는 안뽑힌다. built-in validation set을 제공하는거임.
	- $B$개의 서로 다른 데이터셋을 만들고, 각각으로 decision tree 등을 학습시킨다. 이제, prediction을 합친다. regression은 평균을 취하고, classification은 majority vote를 한다.
	- 모델 $B$와 데이터 $n$이 충분히 크면 샘플링 불확실성이 줄어들고 실제 데이터 분포에 수렴하게 된다. 즉, 분산이 줄어든다.
### 12-3. Random Forest
- $\bar{Z} = \frac{1}{B}\sum Z_b$로 $B$개 tree의 평균을 구한다. 그러면, $Var(\bar Z)=\rho\sigma^2+\frac{1-\rho}{B}\sigma^2$이다.
	- 즉, tree를 아무리 늘려도, variance의 하한선이 존재한다.
	- bagging의 단점은 데이터들의 correlation이 매우 높다는 것이다. (어쨌던간에 같은 샘플에서 가져오는 것이기 때문.)
	- 따라서, 우리는 상관계수인 $\rho$를 줄여야 한다.
- random forest는 각 spli에서 feature의 random subset만 고려한다. 즉, tree의 생김새가 달라지도록 강제하는 것이고, 이는 $\rho$를 획기적으로 줄여, 앙상블 기법이 더 robust하게 만든다.
- 많은 decorrelated tree를 평균내게 된다. 이는 decision boundary를 부드럽게 만들어 일반화 성능을 높인다.