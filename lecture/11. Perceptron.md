### 11-1. Linear Binary Classification
- binary classification problem: 데이터 $(x_i,y_i)$가 주어졌다고 하자.
- hypothesis class: linear classifier $\text{sign}(w^Tx+b)$
- 목적: 데이터를 올바르게 분류하는 $(w,b)$ 쌍을 찾는 것.
	- perceptron algorithm: 이러한 linear classifier를 학슶하는데 사용하는 online metho이다.
 - geometric foundation
	 - margin($\gamma$): boundary와 training point 사이의 최소 거리
	 - radius($R$): 원점을 중심으로, 모든 데이터를 포함하고 있는 가장 작은 ball의 반지름
 - update rule: 만약 잘못 classified된 것들이 있다면, $w_{t+1}=w_t+x$로 업데이트한다.
	 - 기하적으로, 이는 잘못 에측한 점을 w에 더해서 새로운 w를 얻는 과정이다.
	 - 기존의 true label이 +1, predicted label이 -1이면, $w_t \cdot x < 0$이다. 이는 두 벡터 사이의 angle이 90도 이상이라는 의미이다.
	 - 이제, $w_{t+1} \cdot x$는 양수가 된다. 이 말은 두 벡터 사이의 angle이 90도 미만이라는 말이다.
- online learning setup: 데이터에 대해, 만약 예측한 현재 값이랑 실제 값이 다르면, 위의 update rule을 적용한다.
	- 더 정확히는 $w_{t+1}=w_t+y_tx_t$, $b_{t+1}=b_t+y_t$로 변경한다.
	 - update $b$: bias $b$의 역할은 결정 경계를 원점에서 멀어지게 하는 것이다.
		 - $x'=[1,x_1,\cdots,x_d]$라고 하자. 그러면 $w'=[b,w_1,\cdots,w_d]$에 대해 $w^Tx+b=w'x'$이다. 즉, bias는 차원을 하나 추가하는 역할이라고 볼 수 있다. (이건 모든 것을 inner product로 표현하는 방법임)
### 11-2. convergence proof
 Theorem. Let $D$: training set. Assume:
 1. separability: there exists a $w^*$ and a margin $\gamma>0$ s.t. $y_i(w^*\cdot x_i)\ge\gamma$ for all $i$.
 2. boundedness: all input have bounded norm, $||x_i||\le R$.
Then, the perceptron alg. makes at most $M$ mistakes, where
$$
M \le \left(\frac{R}{\gamma}\right)^2
$$
- visualization: 이상적인 방향에 점점 가까워지면서, 진화한다.
	- 현재 가중치를 이상적인 방향에 projection했을 때, margin은 이러한 projection의 값이 예측 실패 횟수 $M$에 대해,  $M\cdot \gamma$로 커짐을 보장한다.
- 한계: non-linearly separable data의 경우 alg.가 무한히 돌게 된다.
