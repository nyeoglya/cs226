### 13-1. Gradient Descent
- boosting: weak learner를 합쳐서 bias가 작은 하나의 strong learner를 만드는 과정
	- weak learner: random guessing보다 약간 성능이 좋은 classifier
	- strong learner: 임의의 낮은 error를 갖는 classifier
- weak learner $h_t \in H$가 주어졌을 때, 우리는 강한 모델 $H(x)=\sum \alpha_t h_t(x)$를 정의할 수 있다.
	- 여기서 $\alpha_t$는 해당 weak learner에 대응되는 가중치임.
- 목적은 $\min_H \sum Loss(H(x_i),y_i)$이다. 즉, 모든 loss를 최소화하는 모델임.
- gradient boosting에서는 함수 하나를 벡터로 본다. 즉, 함수 $H$가 주어졌을 때, 대응되는 벡터 $(H(x_1),\cdots,H(x_n))$을 생각한다. 그러면, 경사하강법을 적용할 수 있다.
- 여기에는 GD를 사용한다. 이때, 근사된 gradient를 사용하는데 boosting에서 우리가 다루는 건 함수이기 때문에 이론적인 방향과 가장 유사하게 움직이는 새로운 모델을 찾아야 하기 때문이다.
- 이 가짜 방향은 진짜 gradient와 같은 방향 성분과, gradient와 직교하는 성분으로 쪼갤 수 있다. 진짜 gradient와 같은 방향 성분만이 영향을 주지만, 직교하는 성분은 아무런 영향이 없기 때문에 상관없다.
### 13-2. Coordinate Descent
- coordinate descent: standard basis 방향으로만 움직이는 descent.
- 알고리즘: full gradient를 계산하고, 가장 큰 magnitude를 갖는 coordinate를 찾아서 그 방향으로 1만큼 움직인다.
	- 이렇게 한 칸씩 움직이는 과정이 boosting에서 weak learner를 추가하는 과정이 된다.
- CD에서 한 축은 boosting에서의 하나의 weak learner이다.
	- 즉, $h_i$를 하나씩 벡터 $y$에 더해나가면서 최적의 $y$를 찾는 것이다.
	- 이때, 앞에 붙는 가중치 $\alpha_i = \text{argmin}_\alpha L(H_{i-1}+\alpha h_i)$이다.
	- 즉, 각 과정에서 손실을 최소화하는 가중치를 계산한다.
- 각 단계에서 축을 찾는 과정이 weak learner를 학습시키는 과정이다. 이론적으로는 $-\nabla L$과 가장 비슷한 방향을 가진 $h$를 찾는다. (내적을 최대화하는 $h$)
- 그러나, 실제로는 그게 불가능하기 때무넹 데이터를 여러가지 조건으로 분할해서 오차 합이 얼마나 줄어드는지를 계산한다.
### 13-3. Loss Functions
- square loss가 항상 최적은 아니다. $L_{squ}=\sum_i (y_i - H(x_i))^2$라고 하자. 그러면, $\frac{\partial L}{\partial H(x_i)}=H(x_i)-y_i$이다.
	- margin $m_i = y_iH(x_i)$이다. 이 margin을 최대화하는 것이 목적임.
	- 보통 classification은 $m_i$의 부호만 보고, $H$ 값을 정확하게 $y$에 맞추려고 하지는 않는다.
	- 따라서, 수치적으로 값이 다르면 이미 $H$가 올바르게 작동해도 불필요하게 수정하려고 한다.
- adaboost: exponential loss를 사용한다. $L_{exp}=\sum_i e^{-m_i}$이다.
	- margin이 음수이거나, 0에 가까운 데이터에 대해 매우 큰 가중치를 부여한다.
	- 반면, margin이 양수(이미 잘 맞추는 경우)에는 가중치가 작아진다.
	- exp loss를 $H(x_i)$에 대해 편미분해서 가중치를 구한다. 이것이 모델이 해당 데이터에 얼마나 집중해야 하는지를 결정한다. 이 가중치는 normalize한다.
	- $h_t$에 대응되는 weight를 normalize한 가중치를 이용해 계산한 error로 정의하는 것이 핵심이다.
