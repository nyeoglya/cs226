### 3-1. Motivation
- NN: nearest neighbor
- k-NN 알고리즘에 들어가는 입력값: $x_1,x_2,\cdots,x_n \in \mathbb{R}^d$ with $q \in \mathbb{R}^d$, distance function $d$
- k-NN 알고리즘은 다음의 문제를 푸는 것이 목적이다.
$$\text{argmin}_j d(x_j,q)$$
```pseudo
min = d(x[1],q)
argmin = 1
for i=2 to n:
	if min > d(x[i], q):
		min = d(x[i], q)
		argmin = i
return argmin
```
- 시간복잡도: $O(n \times d)$
- 위 알고리즘(1-NN)을 효율적으로 k개를 찾도록 할 수 있는 방법은 무엇일까?
- 만약 정렬을 한다면 시간복잡도는 $O(n+n\log n) = O(n\log n)$이다.
	- n개의 NN에 대해 관심이 있다면(극악의 경우), 이 경우가 optimal하다.
	- 그러나, 보통은 $k<<n$인 k에 대해 관심이 있는 경우가 많다.
- 결과적으로, k-NN 알고리즘의 시간복잡도는 $O(n\log k)$이다. 꼭 모든 데이터를 정렬할 필요는 없고, top-K만 알고 싶은 것이다.
### 3-2. k-NN
- max-heap을 쓴다. 최댓값 찾기는 $O(1)$, 값 교체는 $O(\log k)$이다.
- 즉, 우리가 위에서 원하던 시간복잡도를 heap을 씀으로서 달성했다.
#### 3-2-1. k-NN for Classification
- class $\mathcal{C}$가 있다고 하자. 그러면, 주어진 q에 대해 k개 neighbor를 얻을 수 있다. 그러면 q를 class의 각 원소 중 어디에 속해야 하는지 가장 가까운 여러 개 원소를 관찰함으로써 얻을 수 있다.
#### 3-2-2. k-NN for Regression
- neighbor의 weighted mean을 계산한다. 그러면 평균 값으로 regression을 할 수 있다.
### 3-3. extended k-NN
- 저차원에서는 시간복잡도를 $O(\log n + k)$로 줄일 수 있다.
	- 가령, 1차원이면 binary search를 해서 데이터를 줄이고, 그걸 옆에 있는 k개를 전부 검사하면 된다.
- K-Dimensional tree(KD-tree): 작은 d에 대해 매우 효과적인 데이터구조. ($d\leq10$)
- kd-tree를 만들다가 처음 주어진 특정 개수 이하의 node를 만드는 순간 멈춘다. 그러면 leaf에 여러 개의 노드가 자리잡는다. 이 노드들과 q의 거리를 계산할 때, 위에서 설명한 알고리즘을 다시 써야 한다.
- 각 박스랑 q의 거리를 계산하는데, 만약 그 거리가 너무 크면 그 안의 노드들의 거리는 볼 필요도 없음. 그러면 그 박스 내부를 통째로 버리면 됨. 이를 위해 새로운 distance를 정의한다.
- [내용의 이해를 돕기 위한 자료. 이것도 정리해서 넣기](https://www.geeksforgeeks.org/machine-learning/introductory-guide-to-information-retrieval-using-knn-and-kdtree/)
- $d(\text{RootNode}, q)=0$이다. root node는 q를 포함하는 무한한 크기의 상자이기 때문. 일단 root node를 heap에 집어넣고, for문을 시작한다.
- heap에서 값 하나 pop하고, 그게 leaf node인지 여부를 체크한다.
	-  leaf node가 아니면?
		1. 데이터를 쪼갬으로써 tree를 expand한다. (기준은 중앙값임)
		2. 쪼개진 box를 기준으로 거리를 계산해서 heap에 다시 넣는다.
	-  leaf node면?
		1. 이게 가장 가까운 데이터이다.
- (위는 PPT에 있는 알고리즘과는 다르다. heap을 여러 개 쓰는 PPT 알고리즘이 약간 더 이득이 있지만, 개념 이해를 위해서는 위가 더 좋다)
	- PPT에서의 PQ_node는 모든 node를 포함하고, PQ_nbrs는 leaf node만 포함한다.
- 저차원에서만 효과적인 이유: 데이터의 1%를 샘플링한다고 하자. 차원이 높아질수록 1%에 해당하는 cube의 한 변의 길이가 커진다. 즉, 데이터는 고차원으로 갈수록 sparse해지고, 그럴수록 kNN은 비효율적이다.
### 3-4. ANN
- A: approximately
	- ($1+\epsilon$)-Approximate algorithm.
- 고차원에서 사용가능한 방법이다.
- M: domain, P: set of data points
- 좋은 성질의 neighbor graph G(proximity graph)를 만든다.
	- 만약 우리가 clique(complete graph)를 만들면, 시간복잡도가 $O(n \times d)$이다. 이건 linear search랑 똑같다. 즉, 우리가 원하는 건 상당히 sparse한 그래프이다.
- DiskANN은 proximity graph를 만드는 하나의 알고리즘이다. proximity graph는 종류가 많음.
#### 3-4-1. Best-First Search
- 주어진 그래프와 주어진 점에 대해, open이라는 heap에 후보들을 저장한다. 그리고 heap을 주어진 점과의 거리로 관리하며 최소인 점을 하나씩 추출한다.
#### 3-4-2. Beam Search
- 약간 greedy 알고리즘이다.
- 각 level마다 b개의 candidate를 유지한다.
- 그리고 확장한다. $b=1$이면, greedy이다. $b=\infty$면 BFS(넓이 우선 탐색)다.
#### 3-4-3. Out-Edge Selection
- 각 $p \in P$에 대해,
	- $P \setminus \{p\}$를 $D(p, \cdot)$으로 정렬한다.
	- 모든 $u \in P \setminus \{p\}$에 대해, $p \to u$를 만들고, 모든 $D(u,v) < \frac{1}{\alpha}D(p,v)$인 모든 $v \neq u$를 지운다.
		- 만약, v가 p랑 아주 가까우면, v에서 iteration을 돌 때 자연스럽게 u → v → p보다 더 가까운 경로가 생기기 때문에, u → v를 애초에 만들지 않는 것이다.
- 이 알고리즘은 $\alpha > 1 + \frac{2}{\epsilon}$으로 선택하면 ($1+\epsilon$)-ANN search가 임의의 $q \in M$에 대해 가능함을 보장한다.
#### 3-4-4. Contraction Factor $\alpha$
- $D(u,q)<D(p,q)$이면, p에 비해 u는 개선된 점이다. contraction lemma는 $\phi(\alpha, \epsilon)<1$이라는 것을 통해 greedy 알고리즘이 항상 개선되는 점을 제공함을 보장한다. 이 조건을 위해 필요한 $\alpha > 1 + \frac{2}{\epsilon}$이다.
- 이게 가장 중요한 파트임.
$$
\begin{align}
D(u,q) &\leq D(u,p^*) + D(p^*,q)\\
&\leq \frac{1}{\alpha}D(p,p^*)+D(p^*,q)\\
&\leq \frac{1}{\alpha}(D(p,q)+D(p^*,q))+D(p^*,q)\\
&= \frac{1}{\alpha}D(p,q)+(1+\frac{1}{\alpha})D(p^*,q)\quad(\text{end of the proof})\\
&< \frac{1}{\alpha}D(p,q)+(\frac{1}{1+\epsilon})D(p,q)\\
\end{align}
$$
#### 3-4-5. Termination & Correction
- shortcut property가 있는 유한 그래프 $G=(P,E)$에 대해, 임의의 $q\in\mathcal{M}$를 가지고 P에서 greedy하게 찾으면 언제나 ($1+\epsilon$)-ANN에 멈춘다.